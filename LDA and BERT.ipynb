{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3c26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data from CSV file\n",
    "# data = pd.read_csv('/Users/mohamedgani/Downloads/Exhibits/combined_file.csv')\n",
    "data = pd.read_csv('/Users/mohamedgani/Downloads/cluster_0_1_2.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b4e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>b title</th>\n",
       "      <th>corresponding p text</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_2</th>\n",
       "      <th>preprocessed p text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/127501...</td>\n",
       "      <td>Design of Due Diligence Measures</td>\n",
       "      <td>The Company has established management systems...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>company established management system due dili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/127501...</td>\n",
       "      <td>Due Diligence Measures Taken</td>\n",
       "      <td>The Company first identified suppliers and als...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>company first identified supplier also analyze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/127501...</td>\n",
       "      <td>Due Diligence Results</td>\n",
       "      <td>Through its efforts, the Company received info...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>effort company received information 26 supplie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/320193...</td>\n",
       "      <td>Upstream Due Diligence</td>\n",
       "      <td>Each year, we analyze incident data provided b...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>year analyze incident data provided itsci rcs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/320193...</td>\n",
       "      <td>OECD Step 5: Report on Supply Chain Due Diligence</td>\n",
       "      <td>Apple reports annually on its due diligence re...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>apple report annually due diligence requiremen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8738</th>\n",
       "      <td>11264</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/704532...</td>\n",
       "      <td>Due Diligence Process</td>\n",
       "      <td>Due Diligence Process The Company’s supply cha...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>due diligence process company ’ supply chain r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8739</th>\n",
       "      <td>11264</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/704532...</td>\n",
       "      <td>Design of Due Diligence</td>\n",
       "      <td>The Company’s due diligence measures have been...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>company ’ due diligence measure designed confo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8740</th>\n",
       "      <td>11264</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/704532...</td>\n",
       "      <td>Description of Due Diligence Measures</td>\n",
       "      <td>The\\nCompany’s due diligence on the source and...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>company ’ due diligence source chain custody c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8741</th>\n",
       "      <td>11264</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/704532...</td>\n",
       "      <td>Results of Due Diligence</td>\n",
       "      <td>Identify and Assess Risk in the Supply Chain I...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>identify ass risk supply chain identification ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8742</th>\n",
       "      <td>11264</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/704532...</td>\n",
       "      <td>Results of Due Diligence</td>\n",
       "      <td>Identified Smelters and Refiners.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>identified smelter refiner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8743 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                url  \\\n",
       "0         0  https://www.sec.gov/Archives/edgar/data/127501...   \n",
       "1         0  https://www.sec.gov/Archives/edgar/data/127501...   \n",
       "2         0  https://www.sec.gov/Archives/edgar/data/127501...   \n",
       "3         1  https://www.sec.gov/Archives/edgar/data/320193...   \n",
       "4         1  https://www.sec.gov/Archives/edgar/data/320193...   \n",
       "...     ...                                                ...   \n",
       "8738  11264  https://www.sec.gov/Archives/edgar/data/704532...   \n",
       "8739  11264  https://www.sec.gov/Archives/edgar/data/704532...   \n",
       "8740  11264  https://www.sec.gov/Archives/edgar/data/704532...   \n",
       "8741  11264  https://www.sec.gov/Archives/edgar/data/704532...   \n",
       "8742  11264  https://www.sec.gov/Archives/edgar/data/704532...   \n",
       "\n",
       "                                                b title  \\\n",
       "0                      Design of Due Diligence Measures   \n",
       "1                          Due Diligence Measures Taken   \n",
       "2                                 Due Diligence Results   \n",
       "3                                Upstream Due Diligence   \n",
       "4     OECD Step 5: Report on Supply Chain Due Diligence   \n",
       "...                                                 ...   \n",
       "8738                              Due Diligence Process   \n",
       "8739                            Design of Due Diligence   \n",
       "8740              Description of Due Diligence Measures   \n",
       "8741                           Results of Due Diligence   \n",
       "8742                           Results of Due Diligence   \n",
       "\n",
       "                                   corresponding p text  cluster  cluster_2  \\\n",
       "0     The Company has established management systems...        2          0   \n",
       "1     The Company first identified suppliers and als...        2          1   \n",
       "2     Through its efforts, the Company received info...        2          0   \n",
       "3     Each year, we analyze incident data provided b...        2          1   \n",
       "4     Apple reports annually on its due diligence re...        2          2   \n",
       "...                                                 ...      ...        ...   \n",
       "8738  Due Diligence Process The Company’s supply cha...        2          1   \n",
       "8739  The Company’s due diligence measures have been...        2          0   \n",
       "8740  The\\nCompany’s due diligence on the source and...        2          1   \n",
       "8741  Identify and Assess Risk in the Supply Chain I...        2          0   \n",
       "8742                 Identified Smelters and Refiners.         2          0   \n",
       "\n",
       "                                    preprocessed p text  \n",
       "0     company established management system due dili...  \n",
       "1     company first identified supplier also analyze...  \n",
       "2     effort company received information 26 supplie...  \n",
       "3     year analyze incident data provided itsci rcs ...  \n",
       "4     apple report annually due diligence requiremen...  \n",
       "...                                                 ...  \n",
       "8738  due diligence process company ’ supply chain r...  \n",
       "8739  company ’ due diligence measure designed confo...  \n",
       "8740  company ’ due diligence source chain custody c...  \n",
       "8741  identify ass risk supply chain identification ...  \n",
       "8742                         identified smelter refiner  \n",
       "\n",
       "[8743 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5e1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows where cluster_2 is 0 which is results\n",
    "cluster_1_0 = data[data['cluster_2'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c59d5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>b title</th>\n",
       "      <th>corresponding p text</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_2</th>\n",
       "      <th>preprocessed p text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/127501...</td>\n",
       "      <td>Due Diligence Measures Taken</td>\n",
       "      <td>The Company first identified suppliers and als...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>company first identified supplier also analyze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/320193...</td>\n",
       "      <td>Upstream Due Diligence</td>\n",
       "      <td>Each year, we analyze incident data provided b...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>year analyze incident data provided itsci rcs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/183563...</td>\n",
       "      <td>Due Diligence Performed</td>\n",
       "      <td>Step 1: Establish Strong Company Management Sy...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>step 1 establish strong company management sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/142080...</td>\n",
       "      <td>Due Diligence Program</td>\n",
       "      <td>Design Framework Design Framework We designed ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>design framework design framework designed due...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/142080...</td>\n",
       "      <td>Selected Elements of our Due Diligence Program</td>\n",
       "      <td>The OECD Guidance has established a five-step ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>oecd guidance established fivestep framework d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8733</th>\n",
       "      <td>11258</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/109158...</td>\n",
       "      <td>Section 1: Due diligence framework</td>\n",
       "      <td>In accordance with Rule 13p-1, we undertook du...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>accordance rule 13p1 undertook due diligence e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8734</th>\n",
       "      <td>11258</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/109158...</td>\n",
       "      <td>Section 2: Due diligence measures undertaken</td>\n",
       "      <td>Our due diligence efforts for 2013 included th...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>due diligence effort 2013 included following f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8735</th>\n",
       "      <td>11258</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/109158...</td>\n",
       "      <td>Step 4: Carry out independent third-party audi...</td>\n",
       "      <td>Step 4: Carry out independent third-party audi...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>step 4 carry independent thirdparty audit smel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8738</th>\n",
       "      <td>11264</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/704532...</td>\n",
       "      <td>Due Diligence Process</td>\n",
       "      <td>Due Diligence Process The Company’s supply cha...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>due diligence process company ’ supply chain r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8740</th>\n",
       "      <td>11264</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/704532...</td>\n",
       "      <td>Description of Due Diligence Measures</td>\n",
       "      <td>The\\nCompany’s due diligence on the source and...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>company ’ due diligence source chain custody c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5758 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                                url  \\\n",
       "1         0  https://www.sec.gov/Archives/edgar/data/127501...   \n",
       "3         1  https://www.sec.gov/Archives/edgar/data/320193...   \n",
       "6         8  https://www.sec.gov/Archives/edgar/data/183563...   \n",
       "8        17  https://www.sec.gov/Archives/edgar/data/142080...   \n",
       "9        17  https://www.sec.gov/Archives/edgar/data/142080...   \n",
       "...     ...                                                ...   \n",
       "8733  11258  https://www.sec.gov/Archives/edgar/data/109158...   \n",
       "8734  11258  https://www.sec.gov/Archives/edgar/data/109158...   \n",
       "8735  11258  https://www.sec.gov/Archives/edgar/data/109158...   \n",
       "8738  11264  https://www.sec.gov/Archives/edgar/data/704532...   \n",
       "8740  11264  https://www.sec.gov/Archives/edgar/data/704532...   \n",
       "\n",
       "                                                b title  \\\n",
       "1                          Due Diligence Measures Taken   \n",
       "3                                Upstream Due Diligence   \n",
       "6                               Due Diligence Performed   \n",
       "8                                 Due Diligence Program   \n",
       "9        Selected Elements of our Due Diligence Program   \n",
       "...                                                 ...   \n",
       "8733                 Section 1: Due diligence framework   \n",
       "8734       Section 2: Due diligence measures undertaken   \n",
       "8735  Step 4: Carry out independent third-party audi...   \n",
       "8738                              Due Diligence Process   \n",
       "8740              Description of Due Diligence Measures   \n",
       "\n",
       "                                   corresponding p text  cluster  cluster_2  \\\n",
       "1     The Company first identified suppliers and als...        2          1   \n",
       "3     Each year, we analyze incident data provided b...        2          1   \n",
       "6     Step 1: Establish Strong Company Management Sy...        2          1   \n",
       "8     Design Framework Design Framework We designed ...        2          1   \n",
       "9     The OECD Guidance has established a five-step ...        2          1   \n",
       "...                                                 ...      ...        ...   \n",
       "8733  In accordance with Rule 13p-1, we undertook du...        2          1   \n",
       "8734  Our due diligence efforts for 2013 included th...        2          1   \n",
       "8735  Step 4: Carry out independent third-party audi...        2          1   \n",
       "8738  Due Diligence Process The Company’s supply cha...        2          1   \n",
       "8740  The\\nCompany’s due diligence on the source and...        2          1   \n",
       "\n",
       "                                    preprocessed p text  \n",
       "1     company first identified supplier also analyze...  \n",
       "3     year analyze incident data provided itsci rcs ...  \n",
       "6     step 1 establish strong company management sys...  \n",
       "8     design framework design framework designed due...  \n",
       "9     oecd guidance established fivestep framework d...  \n",
       "...                                                 ...  \n",
       "8733  accordance rule 13p1 undertook due diligence e...  \n",
       "8734  due diligence effort 2013 included following f...  \n",
       "8735  step 4 carry independent thirdparty audit smel...  \n",
       "8738  due diligence process company ’ supply chain r...  \n",
       "8740  company ’ due diligence source chain custody c...  \n",
       "\n",
       "[5758 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_1_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810910ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence(model, token_lists, measure='c_v'):\n",
    "    \"\"\"\n",
    "    Get model coherence from gensim.models.coherencemodel\n",
    "    :param model: Topic_Model object\n",
    "    :param token_lists: token lists of docs\n",
    "    :param topics: topics as top words\n",
    "    :param measure: coherence metrics\n",
    "    :return: coherence score\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                            coherence=measure)\n",
    "    else:\n",
    "        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n",
    "        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                            coherence=measure)\n",
    "    return cm.get_coherence()\n",
    "\n",
    "def get_silhouette(model):\n",
    "    \"\"\"\n",
    "    Get silhouette score from model\n",
    "    :param model: Topic_Model object\n",
    "    :return: silhouette score\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    lbs = model.cluster_model.labels_\n",
    "    vec = model.vec[model.method]\n",
    "    return silhouette_score(vec, lbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286d0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from language_detector import detect_language\n",
    "\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "###################################\n",
    "#### sentence level preprocess ####\n",
    "###################################\n",
    "\n",
    "# lowercase + base filter\n",
    "# some basic normalization\n",
    "def f_base(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: processed string: see comments in the source code for more info\n",
    "    \"\"\"\n",
    "    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n",
    "    # normalization 2: lower case\n",
    "    s = s.lower()\n",
    "    # normalization 3: \"&gt\", \"&lt\"\n",
    "    s = re.sub(r'&gt|&lt', ' ', s)\n",
    "    # normalization 4: letter repetition (if more than 2)\n",
    "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # normalization 5: non-word repetition (if more than 1)\n",
    "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # normalization 6: string * as delimiter\n",
    "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n",
    "    # normalization 7: stuff in parenthesis, assumed to be less informal\n",
    "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
    "    # normalization 8: xxx[?!]. -- > xxx.\n",
    "    s = re.sub(r'\\W+?\\.', '.', s)\n",
    "    # normalization 9: [.?!] --> [.?!] xxx\n",
    "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
    "    # normalization 10: ' ing ', noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # normalization 11: noise text\n",
    "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
    "    # normalization 12: phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "# language detection\n",
    "def f_lan(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: boolean (s is English)\n",
    "    \"\"\"\n",
    "\n",
    "    # some reviews are actually english but biased toward french\n",
    "    return detect_language(s) in {'English', 'French','Spanish','Chinese'}\n",
    "\n",
    "\n",
    "###############################\n",
    "#### word level preprocess ####\n",
    "###############################\n",
    "\n",
    "# filtering out punctuations and numbers\n",
    "def f_punct(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with punct and number filter out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "\n",
    "# selecting nouns\n",
    "def f_noun(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "\n",
    "# typo correction\n",
    "def f_typo(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        if suggestions:\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            pass\n",
    "            # do word segmentation, deprecated for inefficiency\n",
    "            # w_seg = sym_spell.word_segmentation(phrase=word)\n",
    "            # w_list_fixed.extend(w_seg.corrected_string.split())\n",
    "    return w_list_fixed\n",
    "\n",
    "\n",
    "# stemming if doing word-wise\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def f_stem(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with stemming\n",
    "    \"\"\"\n",
    "    return [p_stemmer.stem(word) for word in w_list]\n",
    "\n",
    "\n",
    "# filtering out stop words\n",
    "# create English stop words list\n",
    "\n",
    "stop_words = (list(\n",
    "    set(get_stop_words('en'))\n",
    "    |set(get_stop_words('es'))\n",
    "    |set(get_stop_words('de'))\n",
    "    |set(get_stop_words('it'))\n",
    "    |set(get_stop_words('ca'))\n",
    "    #|set(get_stop_words('cy'))\n",
    "    |set(get_stop_words('pt'))\n",
    "    #|set(get_stop_words('tl'))\n",
    "    |set(get_stop_words('pl'))\n",
    "    #|set(get_stop_words('et'))\n",
    "    |set(get_stop_words('da'))\n",
    "    |set(get_stop_words('ru'))\n",
    "    #|set(get_stop_words('so'))\n",
    "    |set(get_stop_words('sv'))\n",
    "    |set(get_stop_words('sk'))\n",
    "    #|set(get_stop_words('cs'))\n",
    "    |set(get_stop_words('nl'))\n",
    "    #|set(get_stop_words('sl'))\n",
    "    #|set(get_stop_words('no'))\n",
    "    #|set(get_stop_words('zh-cn'))\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f_stopw(w_list):\n",
    "    \"\"\"\n",
    "    filtering out stop words\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word not in stop_words]\n",
    "\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw review texts\n",
    "    :param rw: review to be processed\n",
    "    :return: sentence level pre-processed review\n",
    "    \"\"\"\n",
    "    s = f_base(rw)\n",
    "    if not f_lan(s):\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences\n",
    "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
    "    :param s: sentence to be processed\n",
    "    :return: word level pre-processed review\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = f_punct(w_list)\n",
    "    w_list = f_noun(w_list)\n",
    "    w_list = f_typo(w_list)\n",
    "    w_list = f_stem(w_list)\n",
    "    w_list = f_stopw(w_list)\n",
    "\n",
    "    return w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e77dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import numpy as np\n",
    "#from Autoencoder import *\n",
    "#from preprocess import *\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# def preprocess(docs, samp_size=None):\n",
    "#     \"\"\"\n",
    "#     Preprocess the data\n",
    "#     \"\"\"\n",
    "#     if not samp_size:\n",
    "#         samp_size = 100\n",
    "\n",
    "#     print('Preprocessing raw texts ...')\n",
    "#     n_docs = len(docs)\n",
    "#     sentences = []  # sentence level preprocessed\n",
    "#     token_lists = []  # word level preprocessed\n",
    "#     idx_in = []  # index of sample selected\n",
    "#     #     samp = list(range(100))\n",
    "#     samp = np.random.choice(n_docs, samp_size)\n",
    "#     for i, idx in enumerate(samp):\n",
    "#         sentence = preprocess_sent(docs[idx])\n",
    "#         token_list = preprocess_word(sentence)\n",
    "#         if token_list:\n",
    "#           idx_in.append(idx)\n",
    "#           sentences.append(sentence)\n",
    "#           token_lists.append(token_list)\n",
    "#         print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n",
    "#     print('Preprocessing raw texts. Done!')\n",
    "#     return sentences, token_lists, idx_in\n",
    "def preprocess(docs, samp_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data\n",
    "    \"\"\"\n",
    "    if not samp_size:\n",
    "        samp_size = 100\n",
    "        \n",
    "    docs = docs.reset_index(drop=True)     ####Added this line to resetting and sending to preprocess\n",
    "    print('Preprocessing raw texts ...')\n",
    "    n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    idx_in = []  # index of sample selected\n",
    "    #     samp = list(range(100))\n",
    "    samp = np.random.choice(n_docs, samp_size)\n",
    "    for i, idx in enumerate(samp):\n",
    "        if idx >= n_docs:\n",
    "            continue\n",
    "        sentence = preprocess_sent(docs[idx])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "          idx_in.append(idx)\n",
    "          sentences.append(sentence)\n",
    "          token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists, idx_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1449d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model object\n",
    "class Topic_Model:\n",
    "    def __init__(self, k=9, method='LDA_BERT'):\n",
    "        \"\"\"\n",
    "        :param k: number of topics\n",
    "        :param method: method chosen for the topic model\n",
    "        \"\"\"\n",
    "        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n",
    "            raise Exception('Invalid method!')\n",
    "        self.k = k\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        #         self.stopwords = None\n",
    "        self.cluster_model = None\n",
    "        self.ldamodel = None\n",
    "        self.vec = {}\n",
    "        #self.gamma = 15  # parameter for reletive importance of lda\n",
    "        self.gamma = 30\n",
    "        self.method = method\n",
    "        self.AE = None\n",
    "        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    def vectorize(self, sentences, token_lists, method=None):\n",
    "        \"\"\"\n",
    "        Get vecotr representations from selected methods\n",
    "        \"\"\"\n",
    "        # Default method\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "\n",
    "        # turn tokenized documents into a id <-> term dictionary\n",
    "        self.dictionary = corpora.Dictionary(token_lists)\n",
    "        # convert tokenized documents into a document-term matrix\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        if method == 'TFIDF':\n",
    "            print('Getting vector representations for TF-IDF ...')\n",
    "            tfidf = TfidfVectorizer()\n",
    "            vec = tfidf.fit_transform(sentences)\n",
    "            print('Getting vector representations for TF-IDF. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'LDA':\n",
    "            print('Getting vector representations for LDA ...')\n",
    "            if not self.ldamodel:\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "\n",
    "            def get_vec_lda(model, corpus, k):\n",
    "                \"\"\"\n",
    "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
    "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
    "                \"\"\"\n",
    "                n_doc = len(corpus)\n",
    "                vec_lda = np.zeros((n_doc, k))\n",
    "                for i in range(n_doc):\n",
    "                    # get the distribution for the i-th document in corpus\n",
    "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
    "                        vec_lda[i, topic] = prob\n",
    "\n",
    "                return vec_lda\n",
    "\n",
    "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
    "            print('Getting vector representations for LDA. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'BERT':\n",
    "\n",
    "            print('Getting vector representations for BERT ...')\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
    "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
    "            print('Getting vector representations for BERT. Done!')\n",
    "            return vec\n",
    "\n",
    "             \n",
    "        elif method == 'LDA_BERT':\n",
    "        #else:\n",
    "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
    "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
    "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
    "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
    "            if not self.AE:\n",
    "                self.AE = Autoencoder()\n",
    "                print('Fitting Autoencoder ...')\n",
    "                self.AE.fit(vec_ldabert)\n",
    "                print('Fitting Autoencoder Done!')\n",
    "            vec = self.AE.encoder.predict(vec_ldabert)\n",
    "            return vec\n",
    "\n",
    "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
    "        \"\"\"\n",
    "        Fit the topic model for selected method given the preprocessed data\n",
    "        :docs: list of documents, each doc is preprocessed as tokens\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Default method\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "        # Default clustering method\n",
    "        if m_clustering is None:\n",
    "            m_clustering = KMeans\n",
    "\n",
    "        # turn tokenized documents into a id <-> term dictionary\n",
    "        if not self.dictionary:\n",
    "            self.dictionary = corpora.Dictionary(token_lists)\n",
    "            # convert tokenized documents into a document-term matrix\n",
    "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        ####################################################\n",
    "        #### Getting ldamodel or vector representations ####\n",
    "        ####################################################\n",
    "\n",
    "        if method == 'LDA':\n",
    "            if not self.ldamodel:\n",
    "                print('Fitting LDA ...')\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "                print('Fitting LDA Done!')\n",
    "        else:\n",
    "            print('Clustering embeddings ...')\n",
    "            self.cluster_model = m_clustering(self.k)\n",
    "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
    "            self.cluster_model.fit(self.vec[method])\n",
    "            print('Clustering embeddings. Done!')\n",
    "\n",
    "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
    "        \"\"\"\n",
    "        Predict topics for new_documents\n",
    "        \"\"\"\n",
    "        # Default as False\n",
    "        out_of_sample = out_of_sample is not None\n",
    "\n",
    "        if out_of_sample:\n",
    "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "            if self.method != 'LDA':\n",
    "                vec = self.vectorize(sentences, token_lists)\n",
    "                print(vec)\n",
    "        else:\n",
    "            corpus = self.corpus\n",
    "            vec = self.vec.get(self.method, None)\n",
    "\n",
    "        if self.method == \"LDA\":\n",
    "            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
    "                                                     key=lambda x: x[1], reverse=True)[0][0],\n",
    "                                    corpus)))\n",
    "        else:\n",
    "            lbs = self.cluster_model.predict(vec)\n",
    "        return lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17058436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohamedgani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mohamedgani/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b34c535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "\n",
    "import argparse\n",
    "\n",
    "#def model(): #:if __name__ == '__main__':\n",
    "from datetime import datetime       ### Added for error handling\n",
    "\n",
    "    \n",
    "    \n",
    "method = \"LDA_BERT\"\n",
    "#samp_size = 1000\n",
    "\n",
    "    #samp_size = 500\n",
    "ntopic = 10\n",
    "    \n",
    "    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n",
    "\n",
    "    #parser.add_argument('--fpath', default='/kaggle/working/train.csv')\n",
    "    #parser.add_argument('--ntopic', default=10,)\n",
    "    #parser.add_argument('--method', default='TFIDF')\n",
    "    #parser.add_argument('--samp_size', default=20500)\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "\n",
    "data = cluster_1_0['corresponding p text'] #pd.read_csv('/kaggle/working/train.csv')\n",
    "data = data.fillna('')  # only the comments has NaN's\n",
    "#samp_size = len(data)\n",
    "samp_size = len(data)\n",
    "rws = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n",
      "Clustering embeddings ...\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "Getting vector representations for BERT ...\n"
     ]
    }
   ],
   "source": [
    "sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n",
    "    # Define the topic model object\n",
    "    #tm = Topic_Model(k = 10), method = TFIDF)\n",
    "tm = Topic_Model(k = ntopic, method = method)\n",
    "    # Fit the topic model by chosen method\n",
    "tm.fit(sentences, token_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a2083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241eeff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca22867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b86c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f338e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run splitting the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022da900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the output columns\n",
    "output_df = pd.DataFrame({'sentences': sentences, 'token_lists': token_lists, 'idx_in': idx_in})\n",
    "# Save the output dataframe to a csv file\n",
    "output_df.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/mohamedgani/Downloads/preprocessed_data.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import numpy as np\n",
    "#def model(): #:if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "method = \"LDA_BERT\"\n",
    "#samp_size = 1000\n",
    "\n",
    "    #samp_size = 500\n",
    "ntopic = 10\n",
    "tm = Topic_Model(k = ntopic, method = method)\n",
    "    # Fit the topic model by chosen method\n",
    "    \n",
    "sentence_list = data['sentences'].tolist()\n",
    "token_lists_list = data['token_lists'].tolist()\n",
    "# for lst in data['token_lists']:\n",
    "#     if not all(isinstance(item, str) for item in lst):\n",
    "#         raise ValueError(\"Token lists should be a list of list of strings\")\n",
    "# tm.fit(data['sentence_list'], data['token_lists_list'])\n",
    "tm.fit(sentence_list, token_lists_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7578e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n",
    "    # Define the topic model object\n",
    "    #tm = Topic_Model(k = 10), method = TFIDF)\n",
    "tm = Topic_Model(k = ntopic, method = method)\n",
    "    # Fit the topic model by chosen method\n",
    "tm.fit(sentences, token_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d720de",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"LDA_BERT\"\n",
    "\n",
    "ntopic = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15443712",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cluster_1_0['preprocessed p text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc35210",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5626398",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_size = len(data)\n",
    "rws = data\n",
    "\n",
    "rws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = Topic_Model(k = ntopic, method = method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a050b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5548af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "\n",
    "# Create a BERT transformer model and a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Convert preprocessed text data to tokenized BERT inputs\n",
    "input_ids = []\n",
    "for text in data['preprocessed_text']:\n",
    "    encoded_text = tokenizer.encode(text, add_special_tokens=True)\n",
    "    input_ids.append(encoded_text)\n",
    "input_ids = np.array(input_ids)\n",
    "\n",
    "# Train LDA model on tokenized BERT inputs\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "tf = vectorizer.fit_transform(input_ids)\n",
    "lda_model = LatentDirichletAllocation(n_components=9, max_iter=10, learning_method='online', verbose=1, random_state=42)\n",
    "lda_model.fit(tf)\n",
    "\n",
    "# Assign each text document in dataframe to a cluster\n",
    "clusters = lda_model.transform(tf)\n",
    "df['cluster'] = np.argmax(clusters, axis=1)\n",
    "\n",
    "# Calculate top keywords for each cluster\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "top_keywords = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_keywords.append(\", \".join([feature_names[i] for i in topic.argsort()[:-6:-1]]))\n",
    "\n",
    "# Create final output dataframe\n",
    "output = []\n",
    "for idx, row in df.iterrows():\n",
    "    output.append([idx, row['cluster'], top_keywords[row['cluster']], *clusters[idx], row['update_description']])\n",
    "final_output = pd.DataFrame(output, columns=[\"doc_index\", \"Cluster_Number\", \"Cluster Keywords\", \"Cluster 0\", \"Cluster 1\", \"Cluster 2\", \"Cluster 3\", \"Cluster 4\", \"Cluster 5\", \"Cluster 6\", \"Cluster 7\", \"Cluster 8\", \"Update Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8309c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
